\documentclass[11pt]{article}

\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage[parfill]{parskip}
\usepackage{url}

\usepackage{courier}
\usepackage{upquote}
\usepackage{listings}
\lstset{basicstyle=\small\ttfamily,breaklines=true,framextopmargin=50pt}
\lstset{xleftmargin=15pt}

\begin{document}

\begin{titlepage}
\begin{center}

\includegraphics[scale=0.6]{iRODS-Consortium-Logo.png}\\[10pt]
\large{Whitepaper 2016-001}\\[100pt]

\textbf{\huge iRODS Networking Performance - Draft 2}\\[190pt]

\textbf {\large Authors:}\\[0.2cm]
\large {Terrell G. Russell}\\[0.1cm]
\large {Jason M. Coposky}\\[0.1cm]
\large {Benjamin Keller}\\[0.1cm]

\vspace{50pt}

\textbf{\large Renaissance Computing Institute (RENCI)}\\[10pt]
\textbf{\large University of North Carolina at Chapel Hill}\\

\end{center}
\end{titlepage}

\clearpage
\tableofcontents
\thispagestyle{empty}

\clearpage

\section{Summary}
iRODS 4.1.9 presents a significant improvement over 4.1.8 (and all prior versions).

With proper tuning of the Linux TCP kernel settings, iRODS 4.1.9, released July 28, 2016, represents up
to a 100x speedup over iRODS 4.1.8 at high latency (100ms RTT).

This whitepaper demonstrates the recent gains in configurability and
throughput as well as defines best practice for administrators and organizations using iRODS.

\section{Introduction}

iRODS networking has historically been managed through compile-time \texttt{\#define} settings.  Beginning with
iRODS 4.1.0, these compile-time settings have been extracted and are now controlled
through configuration variables in \texttt{server\_config.json}.

As recently as iRODS 4.1.8, the TCP send and receive window sizes were also set within the compiled iRODS server code.  This
defining of the window sizes had the unfortunate effect of overriding the Linux kernel's TCP auto tuning and severely reducing
overall throughput over higher latency connections.

This whitepaper is the first comprehensive look at how iRODS behaves in both low- and high-latency, high-bandwidth network scenarios.

iRODS 4.1.8, and 4.1.9 networking is compared across five variables: transfer commands, file size, network RTT, TCP maximum buffer size, iRODS buffer size, and parallel threads.

The code to generate this dataset is available and we encourage others to add new datasets to this initial baseline.

\clearpage
\section{Test Setup}

This set of tests were performed on two bare metal computers running CentOS 7.2.  Each machine had 32 processors, 256GB of RAM, and rotational disks capable of writing 190MB/s.  They were installed in the same rack and held a 10Gbps point-to-point connection.

The values used for the $2*6*3*2*3*6 = \Sexpr{2*6*3*2*3*6}$ combinations initially tested are included in Table \ref{variables-tested}.  Each combination was run 3 times.  Median transfer times are reported.

\begin{table}[h]
\centering
\begin{tabular}{ll}
Variable & Value \\
\hline
Transfer Command & iput, iget \\
File Size & 35MB, 100MB, 500MB, 1GB, 5GB, 10GB   \\
Network RTT (delay) & \textasciitilde0ms, 50ms, 100ms \\
TCP Buffers & default, tuned  \\
iRODS Buffers & 4MB, 50MB, 100MB \\
Parallel Threads & Streaming, 2, 3, 4, 8, 16
\end{tabular}
\caption{Independent Variables}
\label{variables-tested}
\end{table}

The network latency was naturally 0.2 milliseconds.  For the purposes of this analysis, 0.2ms was coded as 0ms. Additional artificial network delays of 50ms and 100ms were introduced and managed via the traffic control binary (\texttt{tc}).

\begin{lstlisting}
Add 100ms delay
$ sudo tc qdisc add dev eth1 root netem delay 100ms

Delete the delay
$ sudo tc qdisc del dev eth1 root netem
\end{lstlisting}

The max TCP buffers on each machine were either the default, or they were increased to 100MB.

\begin{lstlisting}
Default TCP Buffers
$ sudo sysctl net.ipv4.tcp_rmem='4096        87380   6291456'
$ sudo sysctl net.ipv4.tcp_wmem='4096        16384   4194304'
$ sudo sysctl net.core.rmem_max=212992
$ sudo sysctl net.core.wmem_max=212992

Tuned TCP Buffers
$ sudo sysctl net.ipv4.tcp_rmem='4096        87380   104857600'
$ sudo sysctl net.ipv4.tcp_wmem='4096        87380   104857600'
$ sudo sysctl net.core.rmem_max=104857600
$ sudo sysctl net.core.wmem_max=104857600
\end{lstlisting}

Each of the five variables were handled by the python test harness.  The python test harness
manipulated the settings on both ends of the connection, transferred a file (\texttt{iput} or \texttt{iget}) of various sizes
(35MB, 100MB, 500MB, 1GB, 5GB, and 10GB) with a varying number of threads (Streaming, 2, 3, 4, 8, 16), and then removed the file.

\begin{lstlisting}
$ iput -N3 5Gfile
\end{lstlisting}

The elapsed time for each transfer was recorded into a comma separated values file (csv).
The graphs were generated with R.  All of the test harness code and graph generation code used
is available at \url{https://github.com/irods/contrib}.


\subsection{iperf3 Baseline}

\texttt{iperf3} was used to get an understanding of the  baseline throughput at \textasciitilde0ms, 50ms, and 100ms.

Each row (in both Sending and Receiving) in Figure \ref{fig:iperf-boxplots} represents 20 minutes of continuous sampling (240 samples at 5 seconds).  The distribution of throughput rates is wide with significant volatility.  Single samples are
not sufficient to characterize the network.

<<iperf-boxplots, echo=FALSE, fig.lp="fig:", fig.cap="iperf3 @ 10Gbps point-to-point, 1-5 TCP Threads, 0-100ms RTT", fig.align="center", out.width="65%", fig.pos="h">>=
d = read.csv("iperf-auto.csv")
df = subset(d, reverse == "0")
dr = subset(d, reverse == "1")
names=c( "1 Thread, ~0ms", "2 Threads, ~0ms", "3 Threads, ~0ms", "4 Threads, ~0ms", "5 Threads, ~0ms",
"1 Thread, 50ms", "2 Threads, 50ms", "3 Threads, 50ms", "4 Threads, 50ms", "5 Threads, 50ms",
"1 Thread, 100ms", "2 Threads, 100ms", "3 Threads, 100ms", "4 Threads, 100ms", "5 Threads, 100ms")

par(mfrow=c(1,2))
col=rainbow(5)
par(mai=c(1,0,0.5,.85));
boxplot(MB_sec ~ threads + total_delay, data=df, names=names, horizontal=TRUE, ylab="", xlab="MBytes/sec", main="Sending", las=1, col=col, frame.plot=FALSE, yaxt='n')
par(mai=c(1,1,0.5,0));
boxplot(MB_sec ~ threads + total_delay, data=dr, names=names, horizontal=TRUE, ylab="", xlab="MBytes/sec", main="Receiving", las=1, col=col, frame.plot=FALSE, yaxt='n')
axis(side=2, at=1:15, names, tick=FALSE, las=1)
@

\texttt{iperf3} utilizes a single port.  Due to this, multiple TCP threads are in contention while negotiating to communicate with the same port.
This can be seen when 1 or 2 TCP Threads sometimes get better performance
than 3, 4, and 5 TCP Threads.  The multiple threads are fighting for the attention of the port.  Once the network has enough additional delay (100ms), then the congestion
algorithm has time to optimize and can improve the throughput with additional threads.

On balance, the effect of an increased RTT dominates the effect of contention and congestion control in the algorithms.  As the RTT increases, the average throughput is reduced.

iRODS parallel transfer opens a separate port for every thread and so does not exhibit this congestion behavior at the port level.  However, if multiple simultaneous
users are using iRODS with a single threaded transfer (Streaming), they will see the same contention as demonstrated by \texttt{iperf}. Each user will be vying for the attention of
the iRODS server running on a single port (1247, by default).  Any congestion penalty will be paid during Streaming in addition to the iRODS protocol framing done for every iRODS Buffer.


\clearpage
\section{iRODS 4.1.8}

iRODS 4.1.8 has a very predictable, but unbalanced, network performance profile.

At low-latency (\textasciitilde0ms), all six panels in Figures \ref{fig:418-iput1}-\ref{fig:418-iget6} behave
relatively identically and move the file at a significant portion of the available connection.

But once a delay is introduced, and the TCP buffers remain at their default settings, the TCP buffers
are constantly maxed out and the network is starved
while each end waits on the other server before sending the next buffer.  The transfer time only decreases
when multiple threads are used to push more data onto the network at a time.

Increasing the iRODS Buffer Size (moving from the left panels to the right panels) has no effect since the Default
TCP Buffers (bottom row) are much too small to take any advantage of a larger iRODS Buffer, and the Tuned TCP Buffers (top row)
take as much advantage as they can, even when the iRODS Buffers are set to the default 4MB.

The streaming scenario is the worst case for all of these panels (with delay) due to the framing done by the iRODS Protocol
on every buffer and the fact that the iRODS Protocol requires a couple round trips for every buffer to be acknowledged.

Across the file sizes (35MB, 100MB, 500MB, 1GB, 5GB, 10GB) for both \texttt{iput} and \texttt{iget} (Figures \ref{fig:418-iput1}-\ref{fig:418-iget6}), the six panels remain consistent and show a strictly linear relationship between file size and transfer time.

\clearpage


<<418-iput, echo=FALSE, fig.lp="fig:", fig.cap="4.1.8 iput, n=3">>=
library(lattice)
raw = read.csv("results-418-iput.csv")

d <- aggregate(raw[,7], raw[,1:5], FUN = median, na.rm=TRUE)
colnames(d)[6] <- "median_seconds"
#summary(d)
d418iput <- d

d$parallel_buffer <- ordered(d$parallel_buffer, levels = c(4,10,25,50,75,100,200,400),
labels = c('4MB iRODS Buffer',
'10MB iRODS Buffer',
'25MB iRODS Buffer',
'50MB iRODS Buffer',
'75MB iRODS Buffer',
'100MB iRODS Buffer',
'200MB iRODS Buffer',
'400MB iRODS Buffer'))
d$tcp_size <- ordered(d$tcp_size, levels = c('default','big'), labels = c('Default TCP Buffers','Tuned TCP Buffers'))
d$N <- ordered(d$N, levels = c(1,2,3,4,8,16), labels = c('Streaming','2 Threads','3 Threads','4 Threads','8 Threads','16 Threads'))
d$MB <- ordered(d$MB, levels = c(35,100,500,1024,5120,10240), labels = c('35MB File','100MB File','500MB File','1GB File','5GB File','10GB File'))

xyplot( median_seconds ~ delay | parallel_buffer + tcp_size + MB, d, groups = N, main = "iRODS 4.1.8 - iput", type = "b", ylab = "Transfer Time (seconds)", xlab = "Network RTT (milliseconds)", auto.key = list(space="top", columns=2, pt.cex=1, cex=.8))
@


<<418-iget, echo=FALSE, fig.lp="fig:", fig.cap="4.1.8 iget, n=3">>=
library(lattice)
raw = read.csv("results-418-iget-tmp.csv")

d <- aggregate(raw[,7], raw[,1:5], FUN = median, na.rm=TRUE)
colnames(d)[6] <- "median_seconds"
#summary(d)
d418iget <- d

d$parallel_buffer <- ordered(d$parallel_buffer, levels = c(4,10,25,50,75,100,200,400),
labels = c('4MB iRODS Buffer',
'10MB iRODS Buffer',
'25MB iRODS Buffer',
'50MB iRODS Buffer',
'75MB iRODS Buffer',
'100MB iRODS Buffer',
'200MB iRODS Buffer',
'400MB iRODS Buffer'))
d$tcp_size <- ordered(d$tcp_size, levels = c('default','big'), labels = c('Default TCP Buffers','Tuned TCP Buffers'))
d$N <- ordered(d$N, levels = c(1,2,3,4,8,16), labels = c('Streaming','2 Threads','3 Threads','4 Threads','8 Threads','16 Threads'))
d$MB <- ordered(d$MB, levels = c(35,100,500,1024,5120,10240), labels = c('35MB File','100MB File','500MB File','1GB File','5GB File','10GB File'))

xyplot( median_seconds ~ delay | parallel_buffer + tcp_size + MB, d, groups = N, main = "iRODS 4.1.8 - iget", type = "b", ylab = "Transfer Time (seconds)", xlab = "Network RTT (milliseconds)", auto.key = list(space="top", columns=2, pt.cex=1, cex=.8))
@




\clearpage
\section{iRODS 4.1.9}

iRODS 4.1.9 presents a significant improvement across the board for higher latency connections.

When the latency is low (\textasciitilde0ms), iRODS 4.1.9 moves files as quickly as the network will allow.

Similar to 4.1.8, once a latency is added to the connection and the TCP Buffers are at their
default settings (bottom row), iRODS can most effectively increase throughput by increasing the number of threads.
This shows the well documented knowledge that the default linux kernel TCP settings are not designed
for high-bandwidth, high-latency connections.  Increasing the iRODS Buffers from 4MB to 50MB does reduce
the transfer time by up to 30\% for the streaming use case as the larger iRODS Buffer fills the available default TCP
Buffers and is optimized by the dynamic kernel auto tuning.

After the TCP Buffers are increased (top row), the throughput is increased dramatically.  The
Tuned TCP Buffers and 4MB iRODS Buffer panel (top left) illustrates what happens when the
iRODS Buffer size is small enough to starve the network and waste the available bandwidth.

For the 500MB and larger file transfers (Figure \ref{fig:419-iput3}-\ref{fig:419-iput6} and Figures \ref{fig:419-iget3}-\ref{fig:419-iget6}), the tuned TCP Buffers panels (top row) present some interesting
inversion.  When the iRODS Buffer is increased to 50MB and 100MB, since the network is fast enough to
support the transfer fairly quickly, the additional overhead of managing more threads is
greater than just sending the file with fewer threads.  When the iRODS Buffer is held to 4MB (left panel), it remains the
limiting factor for the streaming use case and takes the longest time since every buffer is framed by the
iRODS protocol and must wait for the TCP back and forth.

The Tuned TCP Buffers and 100MB iRODS Buffer panel (top right of Figures \ref{fig:419-iput1}-\ref{fig:419-iget6}) demonstrates
the best case scenario and is investigated more thoroughly in Figure \ref{fig:419-10gb-threads}.


\clearpage


<<419-iput, echo=FALSE, fig.lp="fig:", fig.cap="4.1.9 iput, n=3">>=
raw = read.csv("results-419-iput.csv")

d <- aggregate(raw[,7], raw[,1:5], FUN = median, na.rm=TRUE)
colnames(d)[6] <- "median_seconds"
#summary(d)
#d419iput <- d

d$parallel_buffer <- ordered(d$parallel_buffer, levels = c(4,10,25,50,75,100,200,400),
labels = c('4MB iRODS Buffer',
'10MB iRODS Buffer',
'25MB iRODS Buffer',
'50MB iRODS Buffer',
'75MB iRODS Buffer',
'100MB iRODS Buffer',
'200MB iRODS Buffer',
'400MB iRODS Buffer'))
d$tcp_size <- ordered(d$tcp_size, levels = c('default','big'), labels = c('Default TCP Buffers','Tuned TCP Buffers'))
d$N <- ordered(d$N, levels = c(1,2,3,4,8,16), labels = c('Streaming','2 Threads','3 Threads','4 Threads','8 Threads','16 Threads'))
d$MB <- ordered(d$MB, levels = c(35,100,500,1024,5120,10240), labels = c('35MB File','100MB File','500MB File','1GB File','5GB File','10GB File'))

xyplot( median_seconds ~ delay | parallel_buffer + tcp_size + MB, d, groups = N, main = "iRODS 4.1.9 - iput", type = "b", ylab = "Transfer Time (seconds)", xlab = "Network RTT (milliseconds)", auto.key = list(space="top", columns=2, pt.cex=1, cex=.8))
@


<<419-iget, echo=FALSE, fig.lp="fig:", fig.cap="4.1.9 iget, n=3">>=
raw = read.csv("results-419-iget.csv")

d <- aggregate(raw[,7], raw[,1:5], FUN = median, na.rm=TRUE)
colnames(d)[6] <- "median_seconds"
#summary(d)
#d419iget <- d

d$parallel_buffer <- ordered(d$parallel_buffer, levels = c(4,10,25,50,75,100,200,400),
labels = c('4MB iRODS Buffer',
'10MB iRODS Buffer',
'25MB iRODS Buffer',
'50MB iRODS Buffer',
'75MB iRODS Buffer',
'100MB iRODS Buffer',
'200MB iRODS Buffer',
'400MB iRODS Buffer'))
d$tcp_size <- ordered(d$tcp_size, levels = c('default','big'), labels = c('Default TCP Buffers','Tuned TCP Buffers'))
d$N <- ordered(d$N, levels = c(1,2,3,4,8,16), labels = c('Streaming','2 Threads','3 Threads','4 Threads','8 Threads','16 Threads'))
d$MB <- ordered(d$MB, levels = c(35,100,500,1024,5120,10240), labels = c('35MB File','100MB File','500MB File','1GB File','5GB File','10GB File'))

xyplot( median_seconds ~ delay | parallel_buffer + tcp_size + MB, d, groups = N, main = "iRODS 4.1.9 - iget", type = "b", ylab = "Transfer Time (seconds)", xlab = "Network RTT (milliseconds)", auto.key = list(space="top", columns=2, pt.cex=1, cex=.8))
@







\clearpage
\section{Best Practice}

The most consistently fast way to \texttt{iput} and \texttt{iget} files via iRODS 4.1.9 is with 3 Threads, the iRODS Buffer set to 100MB, and Tuned TCP Buffers.

\subsection{Threads}

Figure \ref{fig:419-10gb-threads} investigates the best number of threads to maximize throughput for \texttt{iput} and \texttt{iget}.  The fastest transfer times were most consistently achieved with 3 Threads.

<<419-10gb-threads, echo=FALSE, fig.lp="fig:", fig.cap="4.1.9 w/ Tuned TCP Buffers, 100MB iRODS Buffer, n=10", fig.align="center", out.height="4in", fig.pos="h">>=

raw = read.csv('results-419-iput-balanceddelay-threads.csv')
d <- aggregate(raw[,7], raw[,1:5], FUN = median, na.rm=TRUE)
colnames(d)[6] <- 'median_seconds'
d419iput <- d
d$parallel_buffer <- ordered(d$parallel_buffer, levels = c(4,50,100), labels = c('4MB iRODS Buffer','50MB iRODS Buffer','100MB iRODS Buffer'))
d$tcp_size <- ordered(d$tcp_size, levels = c('default','big'), labels = c('Default TCP Buffers','Tuned TCP Buffers'))
d$N <- ordered(d$N, levels = c(1,2,3,4,8,16), labels = c('Streaming','2 Threads','3 Threads','4 Threads','8 Threads','16 Threads'))
p1 <- xyplot( median_seconds ~ delay | parallel_buffer + tcp_size, d, groups = N, main = "iRODS 4.1.9 - iput - 10GB file", type = "b", ylab = "Transfer Time (seconds)", xlab = "Network RTT (milliseconds)", auto.key = list(space="top", columns=2, pt.cex=1, cex=.8))

# table data
tp.0      <- subset(d, delay == "0")
tp.0m     <- format(round(tp.0$median_seconds, digits=1), nsmall=1)
tp.0b     <- format(round(tp.0$MB / tp.0$median_seconds, digits=0), nsmall=0)
tp.50     <- subset(d, delay == "50")
tp.50m    <- format(round(tp.50$median_seconds, digits=1), nsmall=1)
tp.50b    <- format(round(tp.50$MB / tp.50$median_seconds, digits=0), nsmall=0)
tp.100    <- subset(d, delay == "100")
tp.100m   <- format(round(tp.100$median_seconds, digits=1), nsmall=1)
tp.100b   <- format(round(tp.100$MB / tp.100$median_seconds, digits=0), nsmall=0)

raw = read.csv('results-419-iget-balanceddelay-threads.csv')
d <- aggregate(raw[,7], raw[,1:5], FUN = median, na.rm=TRUE)
colnames(d)[6] <- 'median_seconds'
d419iget <- d
d$parallel_buffer <- ordered(d$parallel_buffer, levels = c(4,50,100), labels = c('4MB iRODS Buffer','50MB iRODS Buffer','100MB iRODS Buffer'))
d$tcp_size <- ordered(d$tcp_size, levels = c('default','big'), labels = c('Default TCP Buffers','Tuned TCP Buffers'))
d$N <- ordered(d$N, levels = c(1,2,3,4,8,16), labels = c('Streaming','2 Threads','3 Threads','4 Threads','8 Threads','16 Threads'))
p2 <- xyplot( median_seconds ~ delay | parallel_buffer + tcp_size, d, groups = N, main = "iRODS 4.1.9 - iget - 10GB file", type = "b", ylab = "Transfer Time (seconds)", xlab = "Network RTT (milliseconds)", auto.key = list(space="top", columns=2, pt.cex=1, cex=.8))

print(p1, split=c(1, 1, 2, 1), more=TRUE)
print(p2, split=c(2, 1, 2, 1))

# table data
tg.0      <- subset(d, delay == "0")
tg.0m     <- format(round(tg.0$median_seconds, digits=1), nsmall=1)
tg.0b     <- format(round(tg.0$MB / tg.0$median_seconds, digits=0), nsmall=0)
tg.50     <- subset(d, delay == "50")
tg.50m    <- format(round(tg.50$median_seconds, digits=1), nsmall=1)
tg.50b    <- format(round(tg.50$MB / tg.50$median_seconds, digits=0), nsmall=0)
tg.100    <- subset(d, delay == "100")
tg.100m   <- format(round(tg.100$median_seconds, digits=1), nsmall=1)
tg.100b   <- format(round(tg.100$MB / tg.100$median_seconds, digits=0), nsmall=0)
@

The partial listing of timings in Table \ref{419-transfer-times} are conservative since they include
some small overhead introduced by the python test harness.  However, they are applied consistently, so relative assessments remain valid.


\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{r|rr|rr|rr||rr|rr|rr}
 & \multicolumn{6}{c||}{\textbf{iput}} & \multicolumn{6}{c}{\textbf{iget}} \\
 & \multicolumn{2}{c|}{\textasciitilde0ms delay} & \multicolumn{2}{c|}{50ms delay} & \multicolumn{2}{c||}{100ms delay} & \multicolumn{2}{c|}{\textasciitilde0ms delay} & \multicolumn{2}{c|}{50ms delay} & \multicolumn{2}{c}{100ms delay} \\
 & seconds & MB/s & seconds & MB/s & seconds & MB/s & seconds & MB/s & seconds & MB/s & seconds & MB/s \\
\hline
 Streaming & \Sexpr{tp.0m[1]} & \Sexpr{tp.0b[1]} & \Sexpr{tp.50m[1]} & \Sexpr{tp.50b[1]} & \Sexpr{tp.100m[1]} & \Sexpr{tp.100b[1]} & \Sexpr{tg.0m[1]} & \Sexpr{tg.0b[1]} & \Sexpr{tg.50m[1]} & \Sexpr{tg.50b[1]} & \Sexpr{tg.100m[1]} & \Sexpr{tg.100b[1]} \\
 2 Threads & \Sexpr{tp.0m[2]} & \Sexpr{tp.0b[2]} & \Sexpr{tp.50m[2]} & \Sexpr{tp.50b[2]} & \Sexpr{tp.100m[2]} & \Sexpr{tp.100b[2]} & \Sexpr{tg.0m[2]} & \Sexpr{tg.0b[2]} & \Sexpr{tg.50m[2]} & \Sexpr{tg.50b[2]} & \Sexpr{tg.100m[2]} & \Sexpr{tg.100b[2]} \\
 3 Threads & \Sexpr{tp.0m[3]} & \Sexpr{tp.0b[3]} & \Sexpr{tp.50m[3]} & \Sexpr{tp.50b[3]} & \Sexpr{tp.100m[3]} & \Sexpr{tp.100b[3]} & \Sexpr{tg.0m[3]} & \Sexpr{tg.0b[3]} & \Sexpr{tg.50m[3]} & \Sexpr{tg.50b[3]} & \Sexpr{tg.100m[3]} & \Sexpr{tg.100b[3]} \\
 4 Threads & \Sexpr{tp.0m[4]} & \Sexpr{tp.0b[4]} & \Sexpr{tp.50m[4]} & \Sexpr{tp.50b[4]} & \Sexpr{tp.100m[4]} & \Sexpr{tp.100b[4]} & \Sexpr{tg.0m[4]} & \Sexpr{tg.0b[4]} & \Sexpr{tg.50m[4]} & \Sexpr{tg.50b[4]} & \Sexpr{tg.100m[4]} & \Sexpr{tg.100b[4]} \\
 8 Threads & \Sexpr{tp.0m[5]} & \Sexpr{tp.0b[5]} & \Sexpr{tp.50m[5]} & \Sexpr{tp.50b[5]} & \Sexpr{tp.100m[5]} & \Sexpr{tp.100b[5]} & \Sexpr{tg.0m[5]} & \Sexpr{tg.0b[5]} & \Sexpr{tg.50m[5]} & \Sexpr{tg.50b[5]} & \Sexpr{tg.100m[5]} & \Sexpr{tg.100b[5]} \\
16 Threads & \Sexpr{tp.0m[6]} & \Sexpr{tp.0b[6]} & \Sexpr{tp.50m[6]} & \Sexpr{tp.50b[6]} & \Sexpr{tp.100m[6]} & \Sexpr{tp.100b[6]} & \Sexpr{tg.0m[6]} & \Sexpr{tg.0b[6]} & \Sexpr{tg.50m[6]} & \Sexpr{tg.50b[6]} & \Sexpr{tg.100m[6]} & \Sexpr{tg.100b[6]} \\
\end{tabular}
}
\caption{Median 4.1.9 Transfer Times and Maximum Sustained Throughput}
\label{419-transfer-times}
\end{table}

\clearpage

\subsection{iRODS Buffer Size}

Figure \ref{fig:419-iput-10gb-N3} investigates the best iRODS Buffer size while using 3 threads (\texttt{-N3}).

The iRODS Buffer had little effect on the transfer time (even at 100ms RTT, the variance was under 2 seconds).  This agrees with the visual analysis of Figures \ref{fig:419-iput1}-\ref{fig:419-iget6} where the top rows were relatively flat and only showed improvement when the iRODS Buffer was increased for Streaming.

Picking 100MB for the iRODS Buffer size gives strong parallel transfer performance and allows Streaming to improve as much as possible.


<<419-iput-10gb-N3, echo=FALSE, fig.lp="fig:", fig.cap="4.1.9 iput w/ Tuned TCP Buffers and 3 threads, n=10", fig.align="center", out.width="68%", fig.pos="h">>=
raw = read.csv("results-419-iput-N3.csv")

d <- aggregate(raw[,7], raw[,1:5], FUN = median, na.rm=TRUE)
colnames(d)[6] <- "median_seconds"
#summary(d)

d$parallel_buffer <- ordered(d$parallel_buffer, levels = c(4,10,25,50,75,100,200,400),
labels = c('4MB iRODS Buffer',
'10MB iRODS Buffer',
'25MB iRODS Buffer',
'50MB iRODS Buffer',
'75MB iRODS Buffer',
'100MB iRODS Buffer',
'200MB iRODS Buffer',
'400MB iRODS Buffer'))
d$tcp_size <- ordered(d$tcp_size, levels = c('default','big'), labels = c('Default TCP Buffers','Tuned TCP Buffers'))
d$N <- ordered(d$N, levels = c(1,2,3,4,8,16), labels = c('Streaming','2 Threads','3 Threads','4 Threads','8 Threads','16 Threads'))

# create dataframe for 10G
d.10G <- subset(d, MB == "10240" & delay < 150)
xyplot( median_seconds ~ delay | tcp_size, d.10G, groups = parallel_buffer, main = "iRODS 4.1.9 - iput - 10GB file", type = "b", ylab = "Transfer Time (seconds)", xlab = "Network RTT (milliseconds)", auto.key = list(space="top", columns=2, pt.cex=1, cex=.8))
@


\subsection{TCP Buffer Size}

Figures \ref{fig:418-iput1}-\ref{fig:419-iget6} show that a larger TCP Buffer Size increases the throughput on high-latency, high-bandwidth connections.  There were no cases where a smaller maximum TCP Buffer Size outperformed the larger TCP Buffer Size.  With auto tuning in the kernel, the operating system will take advantage of the larger buffers when it can.



\clearpage
\section{Comparison}

iRODS 4.1.9 presents a significant improvement over 4.1.8.

Both \texttt{iput} and \texttt{iget} improved and responded similarly under varying conditions.

Tables \ref{tab:speedup-iput} and \ref{tab:speedup-iget} compare the best case scenarios tested, Tuned TCP Buffers and
100MB iRODS Buffer, between 4.1.8 and 4.1.9 (upper right panels of Figures
\ref{fig:418-iput6} and \ref{fig:419-iput6} for \texttt{iput} and Figures \ref{fig:418-iget6} and \ref{fig:419-iget6}
for \texttt{iget}) over a 10Gbps network connection with a 100ms RTT.

The observed speedup is primarily due to iRODS 4.1.9 no longer setting the TCP send and receive window sizes, allowing TCP auto tuning
to handle the window sizes dynamically.

<< echo=FALSE>>=
# 4.1.8 comparison data frame
d418comp <- subset(d418iput, MB == "10240" & tcp_size == "big" & delay == "100" & parallel_buffer == "100")
d418comp$throughput <- d418comp$MB / d418comp$median_seconds
# 4.1.9 comparison data frame
d419comp <- subset(d419iput, MB == "10240" & tcp_size == "big" & delay == "100" & parallel_buffer == "100")
d419comp$throughput <- d419comp$MB / d419comp$median_seconds
# comparison table columns
t.a <- format(round(d418comp$median_seconds, digits=1), nsmall=1)
t.b <- format(round(d418comp$throughput, digits=0), nsmall=0)
t.c <- format(round(d419comp$median_seconds, digits=1), nsmall=1)
t.d <- format(round(d419comp$throughput, digits=0), nsmall=0)
speedup <- format(round(d419comp$throughput / d418comp$throughput, digits=0), nsmall=0)
#summary(d419comp)
@


\begin{table}[h]
\centering
\begin{tabular}{r|rr|rr|r}
 iput & \multicolumn{2}{c}{4.1.8} & \multicolumn{2}{c}{4.1.9} & \\
 & seconds & MB/s & seconds & MB/s & Speedup\\
\hline
 Streaming  &  \Sexpr{t.a[1]}  &  \Sexpr{t.b[1]}  &  \Sexpr{t.c[1]}  &  \Sexpr{t.d[1]}  &  \Sexpr{speedup[1]}x \\
 2 Threads  &  \Sexpr{t.a[2]}  &  \Sexpr{t.b[2]}  &  \Sexpr{t.c[2]}  &  \Sexpr{t.d[2]}  &  \Sexpr{speedup[2]}x \\
 3 Threads  &  \Sexpr{t.a[3]}  &  \Sexpr{t.b[3]}  &  \Sexpr{t.c[3]}  &  \Sexpr{t.d[3]}  &  \Sexpr{speedup[3]}x \\
 4 Threads  &  \Sexpr{t.a[4]}  &  \Sexpr{t.b[4]}  &  \Sexpr{t.c[4]}  &  \Sexpr{t.d[4]}  &  \Sexpr{speedup[4]}x \\
 8 Threads  &  \Sexpr{t.a[5]}  &  \Sexpr{t.b[5]}  &  \Sexpr{t.c[5]}  &  \Sexpr{t.d[5]}  &  \Sexpr{speedup[5]}x \\
16 Threads  &  \Sexpr{t.a[6]}  &  \Sexpr{t.b[6]}  &  \Sexpr{t.c[6]}  &  \Sexpr{t.d[6]}  &  \Sexpr{speedup[6]}x \\

\end{tabular}
\caption{10GB iput w/ Tuned TCP Buffers, 100MB iRODS Buffer, 100ms RTT}
\label{tab:speedup-iput}
\end{table}




<< echo=FALSE>>=
# 4.1.8 comparison data frame
d418comp <- subset(d418iget, MB == "10240" & tcp_size == "big" & delay == "100" & parallel_buffer == "100")
d418comp$throughput <- d418comp$MB / d418comp$median_seconds
# 4.1.9 comparison data frame
d419comp <- subset(d419iget, MB == "10240" & tcp_size == "big" & delay == "100" & parallel_buffer == "100")
d419comp$throughput <- d419comp$MB / d419comp$median_seconds
# comparison table columns
t.a <- format(round(d418comp$median_seconds, digits=1), nsmall=1)
t.b <- format(round(d418comp$throughput, digits=0), nsmall=0)
t.c <- format(round(d419comp$median_seconds, digits=1), nsmall=1)
t.d <- format(round(d419comp$throughput, digits=0), nsmall=0)
speedup <- format(round(d419comp$throughput / d418comp$throughput, digits=0), nsmall=0)
#summary(d419comp)
@


\begin{table}[h]
\centering
\begin{tabular}{r|rr|rr|r}
 iget & \multicolumn{2}{c}{4.1.8} & \multicolumn{2}{c}{4.1.9} & \\
 & seconds & MB/s & seconds & MB/s & Speedup\\
\hline
 Streaming  &  \Sexpr{t.a[1]}  &  \Sexpr{t.b[1]}  &  \Sexpr{t.c[1]}  &  \Sexpr{t.d[1]}  &  \Sexpr{speedup[1]}x \\
 2 Threads  &  \Sexpr{t.a[2]}  &  \Sexpr{t.b[2]}  &  \Sexpr{t.c[2]}  &  \Sexpr{t.d[2]}  &  \Sexpr{speedup[2]}x \\
 3 Threads  &  \Sexpr{t.a[3]}  &  \Sexpr{t.b[3]}  &  \Sexpr{t.c[3]}  &  \Sexpr{t.d[3]}  &  \Sexpr{speedup[3]}x \\
 4 Threads  &  \Sexpr{t.a[4]}  &  \Sexpr{t.b[4]}  &  \Sexpr{t.c[4]}  &  \Sexpr{t.d[4]}  &  \Sexpr{speedup[4]}x \\
 8 Threads  &  \Sexpr{t.a[5]}  &  \Sexpr{t.b[5]}  &  \Sexpr{t.c[5]}  &  \Sexpr{t.d[5]}  &  \Sexpr{speedup[5]}x \\
16 Threads  &  \Sexpr{t.a[6]}  &  \Sexpr{t.b[6]}  &  \Sexpr{t.c[6]}  &  \Sexpr{t.d[6]}  &  \Sexpr{speedup[6]}x \\

\end{tabular}
\caption{10GB iget w/ Tuned TCP Buffers, 100MB iRODS Buffer, 100ms RTT}
\label{tab:speedup-iget}
\end{table}





\section{Conclusion}

iRODS 4.1.9 transfers files up to two orders of magnitude (100x) faster than iRODS 4.1.8.

It is recommended for high-bandwidth, high-latency connections:

\begin{itemize}
\item to use 3 Threads for maximum throughput during parallel transfer.
\item to increase the iRODS Buffer to 100MB.
\item to increase the maximum TCP Buffer Size.
\end{itemize}

\end{document}

