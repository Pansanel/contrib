\documentclass[letter, 11pt]{article}

\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage[parfill]{parskip}
\usepackage{url}

\usepackage{courier}
\usepackage{upquote}
\usepackage{listings}
\lstset{basicstyle=\small\ttfamily,breaklines=true,framextopmargin=50pt}
\lstset{xleftmargin=15pt}

\begin{document}

\begin{titlepage}
\begin{center}

\includegraphics[scale=0.6]{iRODS-Consortium-Logo.png}\\[10pt]
\large{Whitepaper 2016-001}\\[100pt]

\textbf{\huge iRODS Network Performance}\\[190pt]

\textbf {\large Authors:}\\[0.2cm]
\large {Terrell G. Russell}\\[0.1cm]
\large {Jason M. Coposky}\\[0.1cm]
\large {Benjamin Keller}\\[0.1cm]

\vspace{50pt}

\textbf{\large Renaissance Computing Institute (RENCI)}\\[10pt]
\textbf{\large University of North Carolina at Chapel Hill}\\

\end{center}
\end{titlepage}

\clearpage
\tableofcontents
\thispagestyle{empty}

\clearpage

\section{Summary}
iRODS 4.1.9 presents a significant improvement over 4.1.8 (and all prior versions).

With proper tuning of the Linux TCP kernel settings, iRODS 4.1.9, released July 28, 2016, represents up
to a 112x speedup over iRODS 4.1.8 at high latency (100ms RTT).

This whitepaper demonstrates the recent gains in configurability and
throughput as well as defines best practice for administrators and organizations using iRODS.

\section{Introduction}

iRODS networking has historically been managed through compile-time \texttt{\#define} settings.  Beginning with
iRODS 4.1.0, these compile-time settings have been extracted and are now controlled
through configuration variables in \texttt{server\_config.json}.

As recently as iRODS 4.1.8, the TCP send and receive window sizes were also set within the compiled iRODS server code.  This
defining of the window sizes had the unfortunate effect of overriding the Linux kernel's TCP auto tuning and severely reducing
overall throughput over higher latency connections.

This whitepaper is the first comprehensive look at how iRODS behaves in both low- and high-latency, high-bandwidth network scenarios.

iRODS 4.1.8, and 4.1.9 networking is compared across five variables: file size, network RTT, TCP maximum buffer size, iRODS buffer size, and parallel threads.

The code to generate this dataset is available and we encourage others to add new datasets to this initial baseline.

\clearpage
\section{Test Setup}

This set of tests were performed on two bare metal computers running CentOS 7.2.  They were in the same rack and held a 10Gb point-to-point connection.

The values used for the $3*3*2*3*6 = 324$ combinations tested are included in Table \ref{variables-tested}.  Each combination was run 3 times and averaged.

\begin{table}[h]
\centering
\begin{tabular}{ll}
Variable & Value \\
\hline
File Size & 1GB, 5GB, 10GB   \\
Network RTT (delay) & 0ms, 50ms, 100ms \\
TCP Buffers & default, tuned  \\
iRODS Buffers & 4MB, 50MB, 100MB \\
Parallel Threads & Streaming, 2, 3, 4, 8, 16
\end{tabular}
\caption{Independent Variables}
\label{variables-tested}
\end{table}

The network latency was naturally 0.2 milliseconds, but additional artificial network delays 
of 50 and 100 milliseconds were introduced and managed via the traffic control binary (\texttt{tc}).

\begin{lstlisting}
Add 100ms delay
$ sudo tc qdisc add dev eth1 root netem delay 100ms

Delete the delay
$ sudo tc qdisc del dev eth1 root netem
\end{lstlisting}

The max TCP buffers on each machine were either the default, or they were increased to 100MB.

\begin{lstlisting}
Default TCP Buffers
$ sudo sysctl net.ipv4.tcp_rmem='4096        87380   6291456'
$ sudo sysctl net.ipv4.tcp_wmem='4096        16384   4194304'
$ sudo sysctl net.core.rmem_max=212992
$ sudo sysctl net.core.wmem_max=212992

Tuned TCP Buffers
$ sudo sysctl net.ipv4.tcp_rmem='4096        87380   104857600'
$ sudo sysctl net.ipv4.tcp_wmem='4096        87380   104857600'
$ sudo sysctl net.core.rmem_max=104857600
$ sudo sysctl net.core.wmem_max=104857600
\end{lstlisting}

Each of the five variables were handled by the python test harness.  The python test harness
manipulated the settings on both ends of the connection, \texttt{iput} a file of various sizes
(1GB, 5GB, and 10GB) with a varying number of threads (Streaming, 2, 3, 4, 8, 16), and then removed it.

The elapsed time for each \texttt{iput} was recorded into a comma separated values file (csv). 
The graphs were generated with R.  All of the test harness code and graph generation code used
is available at \url{https://github.com/irods/contrib}.

\iffalse
\clearpage
\section{iRODS 3.3.1}

To be added.

iRODS 3.3.1 has not yet been put through these same tests.

It is expected that 3.3.1's performance will be similar to 4.1.8 (see the next section).
\fi

\clearpage
\section{iRODS 4.1.8}

iRODS 4.1.8 has a very predictable, but unbalanced, network performance profile.

At low-latency (0ms), all six panels in Figures 1-3 behave relatively identically and move the file at a significant
portion of the available connection.

But once a delay is introduced, and the TCP buffers remain at their default settings, the TCP buffers
are constantly maxed out and the network is starved
while each end waits on the other server before sending the next buffer.  The transfer time only decreases
when multiple threads are used to push more data onto the network at a time.

Increasing the iRODS Buffer Size (moving from the left panels to the right panels) has no effect since the Default
TCP Buffers (bottom row) are much too small to take any advantage of a larger iRODS Buffer, and the Tuned TCP Buffers (top row)
take as much advantage as they can, even when the iRODS Buffers are set to the default 4MB.

The streaming scenario is the worst case for all of these panels (with delay) due to the framing done by the iRODS Protocol
on every buffer and the fact that the iRODS Protocol requires a couple round trips for every buffer to be acknowldged.

Across the three file sizes (1GB, 5GB, 10GB) (Figures 1-3), the six panels remain consistent and show a strictly linear
relationship between file size and transfer time.

\clearpage
\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{418-418-1GB.png}
    \caption{4.1.8 w/ 1GB file, n=3}
\end{figure}

\clearpage
\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{418-418-5GB.png}
    \caption{4.1.8 w/ 5GB file, n=3}
\end{figure}

\clearpage
\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{418-418-10GB.png}
    \caption{4.1.8 w/ 10GB file, n=3}
    \label{418-10gb}
\end{figure}


\clearpage
\section{iRODS 4.1.9}

iRODS 4.1.9 presents a significant improvement across the board for higher latency connections.

When the latency is low (0ms), iRODS 4.1.9 moves files as quickly as the network will allow.

Similar to 4.1.8, once a latency is added to the connection and the TCP Buffers are at their
default settings (bottom row), iRODS can most effectively increase throughput by increasing the number of threads.
This shows the well documented knowledge that the default linux kernel TCP settings are not designed
for high-bandwidth, high-latency connections.  Increasing the iRODS Buffers from 4MB to 50MB does reduce
the transfer time by up to 30\% for the streaming use case as the larger iRODS Buffer fills the available default TCP
Buffers and is optimized by the dynamic kernel auto tuning.

After the TCP Buffers are increased (top row), the throughput is increased dramatically.  The
Tuned TCP Buffers and 4MB iRODS Buffer panel (top left) illustrates what happens when the
iRODS Buffer size is small enough to starve the network and waste the available bandwidth.

For the 1GB file transfer (Figure \ref{419-1gb}), the tuned TCP Buffers panels (top row) present some interesting
inversion.  When the iRODS Buffer is increased to 50MB and 100MB, since the network is fast enough to 
support completion of a 1GB file fairly quickly, the additional overhead of managing more threads is
greater than just sending the file with fewer threads.  When the iRODS Buffer is held to 4MB, it remains the
limiting factor for the streaming use case and takes the longest time since every buffer is framed by the
iRODS protocol and must wait for the TCP back and forth.

The Tuned TCP Buffers and 100MB iRODS Buffer panel (top right) demonstrates the best case
scenario and is investigated more thoroughly in Figure \ref{best-case}.


\clearpage
\begin{figure}[h]
    \centering
    \includegraphics[width=.95\textwidth]{419-419-1GB.png}
    \caption{4.1.9 w/ 1GB file, n=3}
    \label{419-1gb}
\end{figure}

\clearpage
\begin{figure}[h]
    \centering
    \includegraphics[width=.95\textwidth]{419-419-5GB.png}
    \caption{4.1.9 w/ 5GB file, n=3}
\end{figure}

\clearpage
\begin{figure}[h]
    \centering
    \includegraphics[width=.95\textwidth]{419-419-10GB.png}
    \caption{4.1.9 w/ 10GB file, n=3}
    \label{419-10gb}
\end{figure}


\clearpage
\section{Comparison}

iRODS 4.1.9 presents a significant improvement over 4.1.8.

Table \ref{speedup} compares the best case scenario of Tuned TCP Buffers and
100MB iRODS Buffer between 4.1.8 and 4.1.9 (upper right panels of Figures
\ref{418-10gb} and \ref{419-10gb}) over a network connection with a 100ms RTT.

This speedup is mostly due to iRODS 4.1.9 no longer setting the TCP send and receive window sizes, allowing TCP auto tuning
to handle the window sizes dynamically.

\begin{table}[h]
\centering
\begin{tabular}{r|rr|rr|r}
 & \multicolumn{2}{c}{4.1.8} & \multicolumn{2}{c}{4.1.9} & \\
 & seconds & MB/s & seconds & MB/s & Speedup\\
\hline
 Streaming  &  5150.65   &    1.9   &  45.61  &  224.4  &  112.9x \\
 2 Threads  &   534.90   &   19.1   &  34.30  &  298.5  &   15.6x \\
 3 Threads  &   357.61   &   28.6   &  31.43  &  325.7  &   11.4x \\
 4 Threads  &   268.84   &   38.0   &  37.42  &  273.6  &    7.2x \\
 8 Threads  &   135.83   &   75.3   &  40.44  &  253.1  &    3.4x \\
16 Threads  &    70.26   &  145.7   &  57.21  &  178.9  &    1.2x \\

\end{tabular}
\caption{10GB File Transfer w/ Tuned TCP Buffers, 100MB iRODS Buffer, 100ms RTT}
\label{speedup}
\end{table}




\clearpage
\section{Best Practice}

The most consistently fast way to send files via iRODS 4.1.9 is with Tuned TCP Buffers, iRODS Buffer set to 100MB, and
with 3 Threads (\texttt{iput -N3}).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.68\textwidth]{419-419-10GB-detail.png}
    \caption{4.1.9 w/ Tuned TCP Buffers and 100MB iRODS Buffer, n=10}
    \label{best-case}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{r|rr|rr|rr}
 & \multicolumn{2}{c}{0ms delay} & \multicolumn{2}{c}{50ms delay} & \multicolumn{2}{c}{100ms delay} \\
 & seconds & MB/s & seconds & MB/s & seconds & MB/s \\
\hline
 Streaming  &  21.16  &   483.7  &  27.09  &  377.9  &  45.61  &  224.4 \\
 2 Threads  &  11.83  &   864.8  &  19.25  &  531.7  &  34.30  &  298.5 \\
 3 Threads  &  10.01  &  1022.5  &  19.09  &  536.3  &  31.43  &  325.7 \\ 
 4 Threads  &   9.56  &  1070.4  &  24.02  &  426.3  &  37.42  &  273.6 \\
 8 Threads  &   9.31  &  1099.0  &  33.73  &  303.5  &  40.44  &  253.1 \\
16 Threads  &   9.32  &  1097.9  &  29.41  &  348.1  &  57.21  &  178.9 \\
\end{tabular}
\caption{Median Transfer Times and Maximum Sustained Throughput}
\label{419-transfer-times}
\end{table}

The partial listing of timings in Table \ref{419-transfer-times} are conservative since they include
some small overhead introduced by the python framework.  However, they are applied consistently, so relative assessments remain valid.



\end{document}

